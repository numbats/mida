---
title: "ETC5510: Introduction to Data Analysis"
week: "Week 8, part A"
subtitle: "Text analysis"
author: "Nicholas Tierney & Stuart Lee"
email: "ETC5510.Clayton-x@monash.edu"
date: "May 2020"
pdflink: ""
bgimg: "images/bg1.jpg"
output:
  xaringan::moon_reader:
    css:
      - ninjutsu 
      - "assets/animate.css"
      - "assets/monash-logo.css"
      - "assets/monash-brand.css"
      - "assets/monash-fonts.css"
      - "assets/styles.css" # small improvements
      - "assets/custom.css" # add your own CSS here!
      - "assets/demo.css" # this should be removed
    self_contained: false 
    seal: false 
    chakra: 'libs/remark-latest.min.js'
    lib_dir: libs
    includes:
      in_header: "assets/custom.html"
    mathjax: "assets/mathjax-local/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
    nature:
      highlightStyle: github 
      highlightLanguage: r 
      highlightLines: true
      highlightSpans: false 
      countIncrementalSlides: false
      slideNumberFormat: '%current%/%total%'
      navigation:
        scroll: false 
        touch: true
        click: false
      ratio: '16:9'
---
  
```{r titleslide, child="components/titleslide.Rmd"}
```


```{r setup, include=FALSE}
library(tidyverse)
library(knitr)
library(kableExtra)
library(textdata)
library(tm)

opts_chunk$set(echo = TRUE,   
               out.width = "100%",
               message = FALSE,
               warning = FALSE,
               collapse = TRUE,
               fig.height = 4,
               fig.width = 8,
               fig.align = "center",
               cache = FALSE)

as_table <- function(...) knitr::kable(..., format='html', digits = 3)
```

---
class: refresher
# recap

- linear models (are awesome)
- many models

---
# Announcements

- Assignment
- Project
- Peer review marking

---
# Why text analysis?

- Predict Melbourne house prices from realtor descriptions
- Determine the extent of public discontent with train stoppages in Melbourne
- The differences between Darwin's first edition of the Origin of the Species and the 6th edition
- Does the sentiment of posts on Newcastle Jets public facebook page reflect their win/loss record?

---
# Typical Process

1. Read in text
2. Pre-processing: remove punctuation signs, remove numbers, stop words, stem words
3. Tokenise: words, sentences, ngrams, chapters
4. Summarise
5. model

---
# Packages

In addition to `tidyverse` we will be using three other packages today

```{r list-pkgs}
library(tidytext)
library(gutenbergr)
```

---
# Tidytext

- Using tidy data principles can make many text mining tasks easier, more effective, and consistent with tools already in wide use.
- Learn more at https://www.tidytextmining.com/, by Julia Silge and David Robinson.

---
# What is tidy text?

```{r show-text}
text <- c("This will be an uncertain time for us my love",
          "I can hear the echo of your voice in my head",
          "Singing my love",
          "I can see your face there in my hands my love",
          "I have been blessed by your grace and care my love",
          "Singing my love")

text
```

---
# What is tidy text?

```{r tidy-text-tile}
text_df <- tibble(line = seq_along(text), text = text)

text_df
```

---
# What is tidy text?

```{r unnest-tokens}
text_df %>%
  unnest_tokens(
    output = word,
    input = text,
    token = "words" # default option
  ) 
```

---
# What is unnesting?

```{r unnest-tokens-chars}
text_df %>%
  unnest_tokens(
    output = word,
    input = text,
    token = "characters"
  )
```

---
# What is unnesting - ngrams length 2

```{r unnest-tokens-ngram-2}
text_df %>%
  unnest_tokens(
    output = word,
    input = text,
    token = "ngrams",
    n = 2
  )
```

---
# What is unnesting - ngrams length 3

```{r unnest-tokens-ngram-3}
text_df %>%
  unnest_tokens(
    output = word,
    input = text,
    token = "ngrams",
    n = 3
  )
```

---
class: transition

# Analyzing user reviews for Animal Crossing: New Horizons

---
# About the data

- User and critic reviews for the game [Animal Crossing](https://www.nintendo.com/games/detail/animal-crossing-new-horizons-switch/) scraped from Metacritc

- This data comes from a [#TidyTuesday challenge](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-05-05/readme.md). 



---
# What do the user reviews look like?

```{r, echo = TRUE}
acnh_user_reviews <- read_tsv(here::here("slides/data/acnh_user_reviews.tsv"))
glimpse(acnh_user_reviews)
```


---
# Let's look at the grade distrubtion

```{r review-grades, echo = FALSE, out.width = "90%"}
acnh_user_reviews %>% 
  count(grade) %>% 
  ggplot(aes(x = grade, y = n )) +
  geom_col()
```


---
# Read a few of the positive reviews

.left_code[
```{r pos-reviews, echo = TRUE, eval = FALSE}
set.seed(1999)
acnh_user_reviews %>% 
  filter(grade > 8) %>% 
  sample_n(3) %>% 
  pull(text)
```
]

.pull_right[
```{r ref.label="pos-reviews", eval = TRUE, echo = FALSE}
```
]


---
# And some negative reviews

.left_code[
```{r neg-reviews, echo = TRUE, eval = FALSE }
set.seed(2099)
acnh_user_reviews %>% 
  filter(grade == 0) %>% 
  sample_n(3) %>% 
  pull(text)
```
]

.pull_right[
```{r ref.label="neg-reviews", eval = TRUE, echo = FALSE}
```
]

---
# Looks like the scraping is messed up a bit

Long reviews are compressed from the scraping procedure...
```{r,}
acnh_user_reviews_parsed <- acnh_user_reviews %>% 
  mutate(text = str_remove(text, "Expand$"))
```

We will remove these characters from the text..

---
# Tidy up the reviews!

```{r unnest-tokens-acnh}
user_reviews_words <- acnh_user_reviews_parsed %>%
  unnest_tokens(output = word, input = text)

user_reviews_words
```

---
# Distribution of words per review?

```{r word-histogram, out.width="90%"}
user_reviews_words %>% 
  count(user_name) %>% 
  ggplot(aes(x = n)) +
  geom_histogram()
```

---
# What are the most common words?

```{r common-words}
user_reviews_words %>%
  count(word, sort = TRUE)
```

---
# Stop words

- In computing, stop words are words which are filtered out before or after processing of natural language data (text).
- They usually refer to the most common words in a language, but there is not a single list of stop words used by all natural language processing tools.

---
# English stop words

```{r eng-stopwords}
get_stopwords()
```

---
# Spanish stop words

```{r spanish-stopwords}
get_stopwords(language = "es")
```

---
# Various lexicons

See `?get_stopwords` for more info.

```{r other-lexicons}
get_stopwords(source = "smart")
```

---
# What are the most common words?

```{r repeat}
user_reviews_words %>%
  count(word, sort = TRUE)
```

---
# What are the most common words?

```{r stopwords-anti-join}
stopwords_smart <- get_stopwords(source = "smart")

user_reviews_words %>%
  anti_join(stopwords_smart) 
```

---
## Aside: the anti-join

- A type of filtering join, will return all rows on the left when there
are no matches on the right
- Only keeps columns on the left 

---
## As a picture

```{r animate-anti-join, echo = FALSE, out.width = "50%"}
include_graphics("gifs/anti-join.gif")
```

---
# What are the most common words?

```{r stopwords-anti-join-complete}
user_reviews_words %>%
  anti_join(stopwords_smart) %>%
  count(word, sort = TRUE) 
```

---
# What are the most common words?

```{r gg-common-words, eval=FALSE}
user_reviews_words %>%
  anti_join(stopwords_smart) %>%
  count(word) %>%
  arrange(-n) %>%
  top_n(20) %>%
  ggplot(aes(fct_reorder(word, n), n)) +
  geom_col() +
  coord_flip() +
  theme_minimal() +
  labs(title = "Frequency of words in user reviews",
       subtitle = "",
       y = "",
       x = "")
```

---

```{r gg-common-words-out, ref.label = 'gg-common-words', echo = FALSE, out.width = "100%"}
```

---
# Sentiment analysis

- One way to analyze the sentiment of a text is to consider the text as a combination of its individual words 

- and the sentiment content of the whole text as the sum of the sentiment content of the individual words

---
# Sentiment lexicons

.pull-left[
```{r show-sentiment-afinn}
get_sentiments("afinn")
```
]

.pull-right[
```{r show-sentiment-bing}
get_sentiments("bing")
```
]

---
# Sentiment lexicons

.pull-left[
```{r show-sentiment-bing2}
get_sentiments(lexicon = "bing")
```
]

.pull-right[
```{r show-sentiment-loughran}
get_sentiments(lexicon = "loughran")
```
]

---
# Sentiments in the reviews

```{r sentiment-powderfinger}
sentiments_bing <- get_sentiments("bing")

user_reviews_words %>%
  inner_join(sentiments_bing) %>%
  count(sentiment, word, sort = TRUE) 
```

---
# Visualising sentiments

```{r gg-sentiment, echo=FALSE, message=FALSE}
user_reviews_words %>%
  inner_join(sentiments_bing) %>%
  count(sentiment, word, sort = TRUE) %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  ggplot(aes(fct_reorder(word, n), n, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~sentiment, scales = "free") +
  theme_minimal() +
  labs(
    title = "Sentiments in user reviews",
    x = ""
  )
```

---
# Visualising sentiments

```{r gg-sentiment2, eval = FALSE}
user_reviews_words %>%
  inner_join(sentiments_bing) %>%
  count(sentiment, word, sort = TRUE) %>%
  arrange(desc(n)) %>%
  group_by(sentiment) %>%
  top_n(10) %>%
  ungroup() %>%
  ggplot(aes(fct_reorder(word, n), n, fill = sentiment)) +
  geom_col() +
  coord_flip() +
  facet_wrap(~sentiment, scales = "free") +
  theme_minimal() +
  labs(
    title = "Sentiments in user reviews",
    x = ""
  )
```


---

# Common words over grades "user"

```{r common-user-words-stop}
user_reviews_words %>%
  anti_join(stopwords_smart) %>%
  count(grade, word, sort = TRUE) 
```

---
# Common review words by grade - With stop words:

```{r common-user-words}
user_reviews_words %>%
  count(grade, word, sort = TRUE)
```

---
# What is a document about?

## How do we measure the importance of a word to a document in a collection of documents?

i.e a novel in a collection of novels or a review in a set of reviews...

We combine the following statistics:

- Term frequency
- Inverse document frequency

---

# Term frequency

The raw frequency of a word $w$ in a document $d$. It is a function of the word and the document. 

$$tf(w, d) = \frac{\text{count of w in d}}{\text{total count in d}} $$

---
# Term frequency

For our reviews a document is a single user's review.

```{r doc-example}
document <- user_reviews_words %>% 
    anti_join(stopwords_smart) %>% 
    filter(user_name == "Discoduckasaur")
document
```


---
# Term frequency

The term frequency for each word is the number of times that word occurs
divided by the total number of words in the document.

```{r tf}
tbl_tf <- document %>% 
  count(word, sort = TRUE) %>% 
  mutate(tf = n / sum(n))
tbl_tf %>% 
  arrange(desc(tf))
```

## Inverse-document frequency

The inverse document frequency tells how common or rare a word is accross a collection of documents. It is a function of a word $w$, and the collection of documents $\mathcal{D}$.

$$idf(w, \mathcal{D}) = \log{\left(\frac{\text{size of } \mathcal{D}}{\text{number of documents that contain } w}\right)}$$
---
## Inverse document frequency

For the reviews data set, our collection is all the reviews. You could
compute this in a somewhat roundabout as follows:

```{r idf}
tbl_idf <- user_reviews_words %>% 
    anti_join(stopwords_smart) %>%
    mutate(collection_size = n_distinct(user_name)) %>% 
    group_by(collection_size, word) %>% 
    summarise(times_word_used = n_distinct(user_name)) %>% 
    mutate(freq = collection_size / times_word_used,
           idf = log(freq)) 
arrange(tbl_idf, idf)
```

---
## Putting it together term frequency, inverse document frequency

Multiply tf and idf together. This is a function of a word $w$, a document $d$,
and the collection of documents $\mathcal{D}$:

$$ tf\_idf(w, d, \mathcal{D}) = tf(w,d) \times idf(w, \mathcal{D})$$
High value of $tf\_idf$ that a word has a high frequency within a document
but is quite rare over all documents. Likewise if a word occurs in a lot
of documents idf will be close to zero, so $tf\_idf$ will be small.

---
## Putting it together, tf-idf 

We illustrate the example for a single user review:

```{r tf-idf}
tbl_tf %>% 
    left_join(tbl_idf) %>% 
    select(word, tf, idf) %>% 
    mutate(tf_idf = tf * idf) %>% 
    arrange(desc(tf_idf))
```
---
# Calculating tf-idf: Perhaps not that exciting...

Instead of rolling our own, we can use `tidytext`

```{r calc-tf-idf}
user_reviews_counts <- user_reviews_words %>%
      anti_join(stopwords_smart) %>% 
      count(user_name, word, sort = TRUE) %>% 
      bind_tf_idf(term = word, document = user_name, n = n)

user_reviews_counts
```

---

# What words were important to (a sample of) users that had positive reviews?

```{r gg-tf-idf, echo=FALSE,message=FALSE, fig.height = 5}
pos_reviews <- acnh_user_reviews_parsed %>% 
    select(user_name, grade) %>% 
    filter(grade > 8) %>% 
    sample_n(3)

user_reviews_counts_pos <- user_reviews_counts %>%
  inner_join(pos_reviews, by = "user_name") 

user_reviews_counts_pos %>% 
  group_by(user_name) %>%
  top_n(10, wt = tf_idf) %>%
  ungroup() %>%
  ggplot(aes(fct_reorder(word, tf_idf), tf_idf, fill = user_name)) +
  geom_col(show.legend = FALSE) + 
  coord_flip() +
  facet_wrap(~user_name, ncol = 1, scales = "free") +
  scale_y_continuous() +
  theme_minimal() +
  labs(x = NULL, y = "tf-idf")
```

---
# Your Turn:

- Go to Rstudio and extend the analyses done in the lecture
- This time we will look at critics reviews

---
class: transition

# Part Two: Tidy Text analysis on Books

---
# Getting some books to study

The [Gutenberg project](http://www.gutenberg.org/wiki/Main_Page) provides the text of over 57000 books free online. 

Let's explore The Origin of the Species by Charles Darwin using the `gutenbergr` R package.

--

We need to know the `id` of the book, which means looking this up online anyway. 
- The first edition is `1228`
- The sixth edition is `2009`

---
# Packages used

- We need the `tm` package to remove numbers from the page, and `gutenbergr` to access the books.

```{r load-extra-pkgs}
# The tm package is needed because the book has numbers
# in the text, that need to be removed, and the
# install.packages("tm")
library(tm)
library(gutenbergr)
```

---
# Download darwin

```{r download-darwin}
darwin1 <- gutenberg_download(1228)
darwin1
# remove the numbers from the text
darwin1$text <- removeNumbers(darwin1$text)
```

---
# Tokenize

.pull-left[
- break into one word per line
- remove the stop words
- count the words
- find the length of the words
]

.pull-right[
```{r tokenize-text}
darwin1_words <- darwin1 %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE) %>%
  mutate(len = str_length(word))

darwin1_words
```
]

---
# Analyse tokens

```{r analyse-text}
quantile(
  darwin1_words$n,
  probs = seq(0.9, 1, 0.01)
)
```

---
# Analyse tokens

.left-code[
```{r gg-analyse-text, eval = FALSE}
darwin1_words %>%
  top_n(n = 20, wt = n) %>%
  ggplot(aes(x = n,
             y = fct_reorder(word, n))) +
  geom_point() +
  ylab("")
```  
]

.right-plot[
```{r gg-analyse-text-out, ref.label = 'gg-analyse-text', echo = FALSE, out.width = "100%"}

```
]

---
# Download and tokenize the 6th edition.

```{r dl-second-edition}
darwin6 <- gutenberg_download(2009)

darwin6$text <- removeNumbers(darwin6$text)
```

---
# show tokenized words

```{r table-second-edition}
darwin6_words <- darwin6 %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE) %>%
  mutate(len = str_length(word))

darwin6_words
```

---
# show tokenized words
```{r quantile-second-edition}
quantile(darwin6_words$n, probs = seq(0.9, 1, 0.01))
```

---
# show tokenized words
```{r gg-second-edition}
darwin6_words %>%
  top_n(n = 20,
        wt = n) %>%
  ggplot(aes(x = n, y = fct_reorder(word, n))) + geom_point() +
  ylab("")
```

---
# Compare the word frequency

.left-code[
```{r full-join-darwin}
darwin <- full_join(
  darwin1_words, 
  darwin6_words, 
  by = "word"
  ) %>%
  rename(
    n_ed1 = n.x, 
    len_ed1 = len.x, 
    n_ed6 = n.y, 
    len_ed6 = len.y
  )
```
]

.right-plot[
```{r full-join-darwin-out}
darwin
```
]



---
# plot the word frequency

.left-code[
```{r plot-word-freq, eval = FALSE}
ggplot(darwin, 
            aes(x = n_ed1, 
                y = n_ed6, 
                label = word)) +
  geom_abline(intercept = 0, 
              slope = 1) +
  geom_point(alpha = 0.5) +
  xlab("First edition") + 
  ylab("6th edition") +
  scale_x_log10() + 
  scale_y_log10() + 
  theme(aspect.ratio = 1)
```
]

.right-plot[
```{r plot-word-freq-out, ref.label = 'plot-word-freq', echo = FALSE, out.width = "100%"}

```
]

---
# Your turn:

- Does it look like the 6th edition was an expanded version of the first?
- What word is most frequent in both editions?
- Find some words that are not in the first edition but appear in the 6th.
- Find some words that are used the first edition but not in the 6th.
- Using a linear regression model find the top few words that appear more often than expected, based on the frequency in the first edition. Find the top few words that appear less often than expected. 

---
# Book comparison

Idea: Find the important words for the content of each document by decreasing the weight of commonly used words and increasing the weight for words that are not used very much in a collection or corpus of documents.

---
# Term frequency, inverse document frequency (`tf_idf`).

Helps measure word importance of a document in a collection of documents.

Recall:

$$ tf\_idf(w, d, \mathcal{D}) = tf(w,d) \times idf(w, \mathcal{D})$$


---
# Bind the editions:
```{r bind-rows-darwin}
darwin <- bind_rows("first" = darwin1_words, 
                    "sixth" = darwin6_words,
                    .id = "edition")

darwin
```

---
# Compute tf-idf

```{r darwin-tf-idf}
darwin_tf_idf <- darwin %>% 
  bind_tf_idf(word, edition, n)

darwin_tf_idf %>% 
  arrange(desc(tf_idf))
```

---
# Plot the results for top words

```{r gg-rawin-tf-idf}
gg_darwin_1_vs_6 <-
darwin_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word,
                       levels = rev(unique(word)))) %>%
  group_by(edition) %>%
  top_n(15) %>%
  ungroup() %>%
  ggplot(aes(x = word, 
             y = tf_idf, 
             fill = edition)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, 
       y = "tf-idf") +
  facet_wrap(~edition, 
             ncol = 2, 
             scales = "free") +
  coord_flip() + 
  scale_fill_brewer(palette = "Dark2")
```  

---

```{r gg-rawin-tf-idf-out, echo = FALSE}
gg_darwin_1_vs_6
```

---
# What do we learn?

- Mr Mivart appears in the 6th edition, multiple times

```{r show-mivart}
str_which(darwin6$text, "Mivart")
darwin6[5435, ]
```

---
# What do we learn?

- Prof title is used more often in the 6th edition
- There is a tendency for latin names 
- Mistletoe was mispelled in the 1st edition

---
# Lab exercise (bonus!)

Text Mining with R has an example comparing historical physics textbooks:  *Discourse on Floating Bodies* by Galileo Galilei, *Treatise on Light* by Christiaan Huygens, *Experiments with Alternate Currents of High Potential and High Frequency* by Nikola Tesla, and *Relativity: The Special and General Theory* by Albert Einstein. All are available on the Gutenberg project. 

Work your way through the [comparison of physics books](https://www.tidytextmining.com/tfidf.html#a-corpus-of-physics-texts). It is section 3.4.

---
# Thanks

- Dr. Mine Çetinkaya-Rundel
- Dr. Julia Silge: https://github.com/juliasilge/tidytext-tutorial
- Dr. Julia Silge and Dr. David Robinson: https://www.tidytextmining.com/
- Josiah Parry: https://github.com/JosiahParry/genius

---
  
```{r endslide, child="components/endslide.Rmd"}
```
